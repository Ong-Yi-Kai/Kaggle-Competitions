{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "from flaml import AutoML\n",
    "import optuna\n",
    "import pickle\n",
    "import datatable as dt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, chi2, f_classif\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE LOADING!\n"
     ]
    }
   ],
   "source": [
    "# loading train and test sets\n",
    "#datatables load large datasets faster and more memory efficient\n",
    "train = dt.fread(r\"C:\\Users\\Ong Yi Kai\\Desktop\\Data\\Kaggle competitions\\Tabular Data Oct 2021\\train.csv\").to_pandas()\n",
    "test = dt.fread(r\"C:\\Users\\Ong Yi Kai\\Desktop\\Data\\Kaggle competitions\\Tabular Data Oct 2021\\test.csv\").to_pandas()\n",
    "print('DONE LOADING!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will help to reduce momory \n",
    "# data will be samller with the same value\n",
    "\n",
    "@jit(forceobj=True)\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "        \n",
    "            \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1878.74 MB\n",
      "Memory usage after optimization is: 549.32 MB\n",
      "Decreased by 70.8%\n",
      "Memory usage of dataframe is 938.89 MB\n",
      "Memory usage after optimization is: 273.70 MB\n",
      "Decreased by 70.8%\n"
     ]
    }
   ],
   "source": [
    "#reducing the memory of data types\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate test into ID and Data\n",
    "ID_test = test.iloc[:,:1]\n",
    "X_test = test.iloc[:,1:]\n",
    "#seperate train into features and targets\n",
    "features, target = train.iloc[:,1:-1], train.iloc[:,-1:]\n",
    "#create a validation set\n",
    "X_train, X_train_meta, y_train, y_train_meta = train_test_split(features, target, test_size=0.2, random_state=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise features to mean=0,std=1\n",
    "scaler = StandardScaler(with_mean=True,with_std=True)\n",
    "scaler.fit(X=X_train,y=y_train)\n",
    "\n",
    "X_train = scaler.transform(X=X_train)\n",
    "X_train_meta = scaler.transform(X=X_train_meta)\n",
    "X_test = scaler.transform(X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\sklearn\\base.py:441: UserWarning: X does not have valid feature names, but SelectKBest was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\sklearn\\base.py:441: UserWarning: X does not have valid feature names, but SelectKBest was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\sklearn\\base.py:441: UserWarning: X does not have valid feature names, but SelectKBest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Univariate feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=150)\n",
    "selector.fit(X=features,y=np.ravel(target))\n",
    "#fit selector on data\n",
    "X_train = selector.transform(X_train)\n",
    "X_train_meta = selector.transform(X_train_meta)\n",
    "X_test = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_train_meta = pd.DataFrame(X_train_meta)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_train_meta = pd.DataFrame(y_train_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIGHT GBM!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparamters found using optuna from kaggle discussions\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'verbosity': '-1',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'feature_pre_filter': False,\n",
    "    'lambda_l1': 8.533875942246594,\n",
    "    'lambda_l2': 2.0533270677941314e-06,\n",
    "    'num_leaves': 13,\n",
    "    'feature_fraction': 0.4,\n",
    "    'bagging_fraction': 1.0,\n",
    "    'bagging_freq': 0,\n",
    "    'min_child_samples': 50,\n",
    "    'early_stopping_round': 100,\n",
    "    'num_iterations':1000\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.847194\n",
      "[200]\tvalid_0's auc: 0.85302\n",
      "[300]\tvalid_0's auc: 0.855288\n",
      "[400]\tvalid_0's auc: 0.856304\n",
      "[500]\tvalid_0's auc: 0.856691\n",
      "[600]\tvalid_0's auc: 0.856903\n",
      "[700]\tvalid_0's auc: 0.85687\n",
      "Early stopping, best iteration is:\n",
      "[633]\tvalid_0's auc: 0.856908\n",
      "auc: 0.856908\n",
      "Fold:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.847892\n",
      "[200]\tvalid_0's auc: 0.853573\n",
      "[300]\tvalid_0's auc: 0.855818\n",
      "[400]\tvalid_0's auc: 0.856827\n",
      "[500]\tvalid_0's auc: 0.857263\n",
      "[600]\tvalid_0's auc: 0.857408\n",
      "[700]\tvalid_0's auc: 0.857443\n",
      "[800]\tvalid_0's auc: 0.857484\n",
      "[900]\tvalid_0's auc: 0.857465\n",
      "Early stopping, best iteration is:\n",
      "[823]\tvalid_0's auc: 0.857513\n",
      "auc: 0.857513\n",
      "Fold:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.846142\n",
      "[200]\tvalid_0's auc: 0.8518\n",
      "[300]\tvalid_0's auc: 0.854095\n",
      "[400]\tvalid_0's auc: 0.855177\n",
      "[500]\tvalid_0's auc: 0.855593\n",
      "[600]\tvalid_0's auc: 0.855726\n",
      "[700]\tvalid_0's auc: 0.855732\n",
      "Early stopping, best iteration is:\n",
      "[674]\tvalid_0's auc: 0.855778\n",
      "auc: 0.855778\n",
      "Fold:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.84609\n",
      "[200]\tvalid_0's auc: 0.85193\n",
      "[300]\tvalid_0's auc: 0.854216\n",
      "[400]\tvalid_0's auc: 0.855317\n",
      "[500]\tvalid_0's auc: 0.855802\n",
      "[600]\tvalid_0's auc: 0.855952\n",
      "[700]\tvalid_0's auc: 0.856024\n",
      "[800]\tvalid_0's auc: 0.855994\n",
      "Early stopping, best iteration is:\n",
      "[702]\tvalid_0's auc: 0.85603\n",
      "auc: 0.856030\n",
      "Fold:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.846816\n",
      "[200]\tvalid_0's auc: 0.852035\n",
      "[300]\tvalid_0's auc: 0.854032\n",
      "[400]\tvalid_0's auc: 0.854948\n",
      "[500]\tvalid_0's auc: 0.855331\n",
      "[600]\tvalid_0's auc: 0.855501\n",
      "[700]\tvalid_0's auc: 0.855563\n",
      "[800]\tvalid_0's auc: 0.855584\n",
      "[900]\tvalid_0's auc: 0.855595\n",
      "Early stopping, best iteration is:\n",
      "[880]\tvalid_0's auc: 0.855622\n",
      "auc: 0.855622\n",
      "Fold:5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.846725\n",
      "[200]\tvalid_0's auc: 0.851948\n",
      "[300]\tvalid_0's auc: 0.854015\n",
      "[400]\tvalid_0's auc: 0.854806\n",
      "[500]\tvalid_0's auc: 0.855164\n",
      "[600]\tvalid_0's auc: 0.855236\n",
      "Early stopping, best iteration is:\n",
      "[553]\tvalid_0's auc: 0.855247\n",
      "auc: 0.855247\n",
      "Fold:6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.846752\n",
      "[200]\tvalid_0's auc: 0.852433\n",
      "[300]\tvalid_0's auc: 0.854652\n",
      "[400]\tvalid_0's auc: 0.85564\n",
      "[500]\tvalid_0's auc: 0.856118\n",
      "[600]\tvalid_0's auc: 0.856261\n",
      "Early stopping, best iteration is:\n",
      "[593]\tvalid_0's auc: 0.856263\n",
      "auc: 0.856263\n",
      "Fold:7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.845376\n",
      "[200]\tvalid_0's auc: 0.851306\n",
      "[300]\tvalid_0's auc: 0.853473\n",
      "[400]\tvalid_0's auc: 0.854365\n",
      "[500]\tvalid_0's auc: 0.854756\n",
      "[600]\tvalid_0's auc: 0.854872\n",
      "[700]\tvalid_0's auc: 0.854909\n",
      "[800]\tvalid_0's auc: 0.854868\n",
      "Early stopping, best iteration is:\n",
      "[700]\tvalid_0's auc: 0.854909\n",
      "auc: 0.854909\n",
      "Fold:8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.84893\n",
      "[200]\tvalid_0's auc: 0.8543\n",
      "[300]\tvalid_0's auc: 0.856472\n",
      "[400]\tvalid_0's auc: 0.857353\n",
      "[500]\tvalid_0's auc: 0.857753\n",
      "[600]\tvalid_0's auc: 0.857881\n",
      "[700]\tvalid_0's auc: 0.857913\n",
      "[800]\tvalid_0's auc: 0.857873\n",
      "Early stopping, best iteration is:\n",
      "[711]\tvalid_0's auc: 0.857937\n",
      "auc: 0.857937\n",
      "Fold:9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Ong Yi Kai\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.84698\n",
      "[200]\tvalid_0's auc: 0.852686\n",
      "[300]\tvalid_0's auc: 0.854784\n",
      "[400]\tvalid_0's auc: 0.855737\n",
      "[500]\tvalid_0's auc: 0.85607\n",
      "[600]\tvalid_0's auc: 0.856259\n",
      "[700]\tvalid_0's auc: 0.856292\n",
      "Early stopping, best iteration is:\n",
      "[687]\tvalid_0's auc: 0.856303\n",
      "auc: 0.856303\n"
     ]
    }
   ],
   "source": [
    "#create Kfold object\n",
    "#startified fold keeps the proportion of positive and negative targets the same for all splits\n",
    "folds = StratifiedKFold(n_splits=10,random_state=2021,shuffle=True)\n",
    "\n",
    "#create arrays to store and aggregrate predictions after each fold\n",
    "#aggregating to reduce the varian\n",
    "lgb_test_pred = np.zeros(len(test))\n",
    "lgb_train_meta_pred = np.zeros(len(y_train_meta))\n",
    "\n",
    "for fold,(train_idx, val_idx) in enumerate(folds.split(X_train,y_train)):\n",
    "    print(f'Fold:{fold}')\n",
    "    \n",
    "    #create Dataset objects\n",
    "    training = lgb.Dataset(X_train.iloc[train_idx,:], label=y_train.iloc[train_idx,:])\n",
    "    CV = lgb.Dataset(X_train.iloc[val_idx,:], label=y_train.iloc[val_idx,:])\n",
    "    \n",
    "    #create lgbm model object and train\n",
    "    model_lgbm = lgb.train(\n",
    "            lgb_params, \n",
    "            training,\n",
    "            valid_sets=[CV], \n",
    "            verbose_eval=100, \n",
    "            early_stopping_rounds=100)\n",
    "    \n",
    "    #predictions for train_meta and bagging\n",
    "    train_meta_pred_fold = model_lgbm.predict(X_train_meta)\n",
    "    lgb_train_meta_pred += train_meta_pred_fold/folds.n_splits\n",
    "    \n",
    "    #prediction for test and bagging\n",
    "    test_pred_fold = model_lgbm.predict(X_test)\n",
    "    lgb_test_pred += test_pred_fold/folds.n_splits\n",
    "    \n",
    "    #roc_auc_score using training CV for personal satisfaction XD\n",
    "    cv_pred = model_lgbm.predict(X_train.iloc[val_idx,:])\n",
    "    auc = roc_auc_score(y_train.iloc[val_idx,:],cv_pred)\n",
    "    print(f\"auc: {auc:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CATBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters trained by optuna from kaggle discussions\n",
    "cat_params = {'iterations': 2866,\n",
    " 'od_wait': 3385,\n",
    " 'learning_rate': 0.04280810491488757,\n",
    " 'reg_lambda': 0.32139709692279206,\n",
    " 'subsample': 0.8442605943226449,\n",
    " 'random_strength': 22.468752639603235,\n",
    " 'depth': 4,\n",
    " 'min_data_in_leaf': 31,\n",
    " 'leaf_estimation_iterations': 15\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.066406</td>\n",
       "      <td>0.321289</td>\n",
       "      <td>1.235352</td>\n",
       "      <td>1.167969</td>\n",
       "      <td>-0.828125</td>\n",
       "      <td>-0.969238</td>\n",
       "      <td>-0.158813</td>\n",
       "      <td>0.559082</td>\n",
       "      <td>-1.050781</td>\n",
       "      <td>0.178833</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200195</td>\n",
       "      <td>-0.620605</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>-0.602051</td>\n",
       "      <td>1.206055</td>\n",
       "      <td>1.677734</td>\n",
       "      <td>-0.806152</td>\n",
       "      <td>-0.505859</td>\n",
       "      <td>-0.726562</td>\n",
       "      <td>-0.474365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.956055</td>\n",
       "      <td>-0.938477</td>\n",
       "      <td>-0.302002</td>\n",
       "      <td>0.600586</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>0.268066</td>\n",
       "      <td>-0.046509</td>\n",
       "      <td>-0.701172</td>\n",
       "      <td>-0.988281</td>\n",
       "      <td>1.758789</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200195</td>\n",
       "      <td>1.610352</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>-0.602051</td>\n",
       "      <td>1.206055</td>\n",
       "      <td>-0.596191</td>\n",
       "      <td>1.241211</td>\n",
       "      <td>-0.505859</td>\n",
       "      <td>-0.726562</td>\n",
       "      <td>-0.474365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.350342</td>\n",
       "      <td>3.398438</td>\n",
       "      <td>0.609863</td>\n",
       "      <td>-0.766602</td>\n",
       "      <td>-0.089783</td>\n",
       "      <td>0.551758</td>\n",
       "      <td>1.128906</td>\n",
       "      <td>0.287109</td>\n",
       "      <td>-1.049805</td>\n",
       "      <td>0.676758</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200195</td>\n",
       "      <td>1.610352</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>-0.602051</td>\n",
       "      <td>-0.829102</td>\n",
       "      <td>-0.596191</td>\n",
       "      <td>-0.806152</td>\n",
       "      <td>-0.505859</td>\n",
       "      <td>-0.726562</td>\n",
       "      <td>-0.474365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.547852</td>\n",
       "      <td>-0.970703</td>\n",
       "      <td>0.088257</td>\n",
       "      <td>-1.242188</td>\n",
       "      <td>-0.463379</td>\n",
       "      <td>0.414551</td>\n",
       "      <td>0.028366</td>\n",
       "      <td>-0.657715</td>\n",
       "      <td>-1.072266</td>\n",
       "      <td>-0.387939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833008</td>\n",
       "      <td>1.610352</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>-0.602051</td>\n",
       "      <td>1.206055</td>\n",
       "      <td>-0.596191</td>\n",
       "      <td>-0.806152</td>\n",
       "      <td>1.977539</td>\n",
       "      <td>-0.726562</td>\n",
       "      <td>-0.474365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.504883</td>\n",
       "      <td>-0.313232</td>\n",
       "      <td>0.026428</td>\n",
       "      <td>-1.080078</td>\n",
       "      <td>1.206055</td>\n",
       "      <td>0.919434</td>\n",
       "      <td>1.008789</td>\n",
       "      <td>1.098633</td>\n",
       "      <td>-0.171021</td>\n",
       "      <td>-0.216187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833008</td>\n",
       "      <td>1.610352</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>1.660156</td>\n",
       "      <td>-0.829102</td>\n",
       "      <td>-0.596191</td>\n",
       "      <td>1.241211</td>\n",
       "      <td>-0.505859</td>\n",
       "      <td>-0.726562</td>\n",
       "      <td>-0.474365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799994</th>\n",
       "      <td>-1.808594</td>\n",
       "      <td>-0.960449</td>\n",
       "      <td>-0.606934</td>\n",
       "      <td>0.029099</td>\n",
       "      <td>-0.815430</td>\n",
       "      <td>0.670410</td>\n",
       "      <td>-0.480469</td>\n",
       "      <td>-2.541016</td>\n",
       "      <td>-0.971191</td>\n",
       "      <td>-0.815918</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200195</td>\n",
       "      <td>-0.620605</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>1.660156</td>\n",
       "      <td>1.206055</td>\n",
       "      <td>1.677734</td>\n",
       "      <td>1.241211</td>\n",
       "      <td>1.977539</td>\n",
       "      <td>1.376953</td>\n",
       "      <td>-0.474365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799995</th>\n",
       "      <td>-1.247070</td>\n",
       "      <td>1.388672</td>\n",
       "      <td>-0.004482</td>\n",
       "      <td>2.894531</td>\n",
       "      <td>0.245850</td>\n",
       "      <td>0.264404</td>\n",
       "      <td>-0.899902</td>\n",
       "      <td>0.381592</td>\n",
       "      <td>-0.208984</td>\n",
       "      <td>0.165161</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200195</td>\n",
       "      <td>1.610352</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>-0.602051</td>\n",
       "      <td>-0.829102</td>\n",
       "      <td>-0.596191</td>\n",
       "      <td>1.241211</td>\n",
       "      <td>-0.505859</td>\n",
       "      <td>-0.726562</td>\n",
       "      <td>-0.474365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799996</th>\n",
       "      <td>-0.552734</td>\n",
       "      <td>-0.935059</td>\n",
       "      <td>-0.189941</td>\n",
       "      <td>-0.191040</td>\n",
       "      <td>1.792969</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.125610</td>\n",
       "      <td>-1.525391</td>\n",
       "      <td>-1.034180</td>\n",
       "      <td>0.433105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833008</td>\n",
       "      <td>-0.620605</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>-0.602051</td>\n",
       "      <td>1.206055</td>\n",
       "      <td>-0.596191</td>\n",
       "      <td>-0.806152</td>\n",
       "      <td>-0.505859</td>\n",
       "      <td>-0.726562</td>\n",
       "      <td>-0.474365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799998</th>\n",
       "      <td>-0.376709</td>\n",
       "      <td>1.443359</td>\n",
       "      <td>-1.616211</td>\n",
       "      <td>0.448242</td>\n",
       "      <td>1.151367</td>\n",
       "      <td>-0.909668</td>\n",
       "      <td>-0.173828</td>\n",
       "      <td>-1.582031</td>\n",
       "      <td>-0.265381</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833008</td>\n",
       "      <td>-0.620605</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>-0.602051</td>\n",
       "      <td>-0.829102</td>\n",
       "      <td>1.677734</td>\n",
       "      <td>1.241211</td>\n",
       "      <td>-0.505859</td>\n",
       "      <td>1.376953</td>\n",
       "      <td>2.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799999</th>\n",
       "      <td>-0.065796</td>\n",
       "      <td>-0.321777</td>\n",
       "      <td>0.200317</td>\n",
       "      <td>1.532227</td>\n",
       "      <td>-0.903320</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>-1.962891</td>\n",
       "      <td>-1.168945</td>\n",
       "      <td>-0.294189</td>\n",
       "      <td>-1.222656</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200195</td>\n",
       "      <td>-0.620605</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>1.660156</td>\n",
       "      <td>-0.829102</td>\n",
       "      <td>1.677734</td>\n",
       "      <td>1.241211</td>\n",
       "      <td>-0.505859</td>\n",
       "      <td>-0.726562</td>\n",
       "      <td>-0.474365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720000 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0      -2.066406  0.321289  1.235352  1.167969 -0.828125 -0.969238 -0.158813   \n",
       "1       0.956055 -0.938477 -0.302002  0.600586  0.568848  0.268066 -0.046509   \n",
       "2      -0.350342  3.398438  0.609863 -0.766602 -0.089783  0.551758  1.128906   \n",
       "3      -0.547852 -0.970703  0.088257 -1.242188 -0.463379  0.414551  0.028366   \n",
       "4       1.504883 -0.313232  0.026428 -1.080078  1.206055  0.919434  1.008789   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "799994 -1.808594 -0.960449 -0.606934  0.029099 -0.815430  0.670410 -0.480469   \n",
       "799995 -1.247070  1.388672 -0.004482  2.894531  0.245850  0.264404 -0.899902   \n",
       "799996 -0.552734 -0.935059 -0.189941 -0.191040  1.792969  0.033600  0.125610   \n",
       "799998 -0.376709  1.443359 -1.616211  0.448242  1.151367 -0.909668 -0.173828   \n",
       "799999 -0.065796 -0.321777  0.200317  1.532227 -0.903320  0.765625 -1.962891   \n",
       "\n",
       "             7         8         9    ...       140       141       142  \\\n",
       "0       0.559082 -1.050781  0.178833  ... -1.200195 -0.620605  0.923828   \n",
       "1      -0.701172 -0.988281  1.758789  ... -1.200195  1.610352  0.923828   \n",
       "2       0.287109 -1.049805  0.676758  ... -1.200195  1.610352  0.923828   \n",
       "3      -0.657715 -1.072266 -0.387939  ...  0.833008  1.610352  0.923828   \n",
       "4       1.098633 -0.171021 -0.216187  ...  0.833008  1.610352  0.923828   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "799994 -2.541016 -0.971191 -0.815918  ... -1.200195 -0.620605  0.923828   \n",
       "799995  0.381592 -0.208984  0.165161  ... -1.200195  1.610352  0.923828   \n",
       "799996 -1.525391 -1.034180  0.433105  ...  0.833008 -0.620605  0.923828   \n",
       "799998 -1.582031 -0.265381 -0.140625  ...  0.833008 -0.620605  0.923828   \n",
       "799999 -1.168945 -0.294189 -1.222656  ... -1.200195 -0.620605  0.923828   \n",
       "\n",
       "             143       144       145       146       147       148       149  \n",
       "0      -0.602051  1.206055  1.677734 -0.806152 -0.505859 -0.726562 -0.474365  \n",
       "1      -0.602051  1.206055 -0.596191  1.241211 -0.505859 -0.726562 -0.474365  \n",
       "2      -0.602051 -0.829102 -0.596191 -0.806152 -0.505859 -0.726562 -0.474365  \n",
       "3      -0.602051  1.206055 -0.596191 -0.806152  1.977539 -0.726562 -0.474365  \n",
       "4       1.660156 -0.829102 -0.596191  1.241211 -0.505859 -0.726562 -0.474365  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "799994  1.660156  1.206055  1.677734  1.241211  1.977539  1.376953 -0.474365  \n",
       "799995 -0.602051 -0.829102 -0.596191  1.241211 -0.505859 -0.726562 -0.474365  \n",
       "799996 -0.602051  1.206055 -0.596191 -0.806152 -0.505859 -0.726562 -0.474365  \n",
       "799998 -0.602051 -0.829102  1.677734  1.241211 -0.505859  1.376953  2.109375  \n",
       "799999  1.660156 -0.829102  1.677734  1.241211 -0.505859 -0.726562 -0.474365  \n",
       "\n",
       "[720000 rows x 150 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.astype('int8')\n",
    "y_train = np.ravel(y_train).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0\n",
      "0:\tlearn: 0.6805051\ttest: 0.6805166\tbest: 0.6805166 (0)\ttotal: 178ms\tremaining: 8m 30s\n",
      "100:\tlearn: 0.5177409\ttest: 0.5182901\tbest: 0.5182901 (100)\ttotal: 16.6s\tremaining: 7m 34s\n",
      "200:\tlearn: 0.5053044\ttest: 0.5060821\tbest: 0.5060821 (200)\ttotal: 32.4s\tremaining: 7m 9s\n",
      "300:\tlearn: 0.4975663\ttest: 0.4986347\tbest: 0.4986347 (300)\ttotal: 48.6s\tremaining: 6m 54s\n",
      "400:\tlearn: 0.4880222\ttest: 0.4894856\tbest: 0.4894856 (400)\ttotal: 1m 5s\tremaining: 6m 44s\n",
      "500:\tlearn: 0.4819444\ttest: 0.4837040\tbest: 0.4837040 (500)\ttotal: 1m 23s\tremaining: 6m 32s\n",
      "600:\tlearn: 0.4782694\ttest: 0.4803071\tbest: 0.4803071 (600)\ttotal: 1m 40s\tremaining: 6m 17s\n",
      "700:\tlearn: 0.4757193\ttest: 0.4779912\tbest: 0.4779912 (700)\ttotal: 1m 56s\tremaining: 6m\n",
      "800:\tlearn: 0.4737457\ttest: 0.4762922\tbest: 0.4762922 (800)\ttotal: 2m 12s\tremaining: 5m 42s\n",
      "900:\tlearn: 0.4721667\ttest: 0.4749949\tbest: 0.4749949 (900)\ttotal: 2m 28s\tremaining: 5m 24s\n",
      "1000:\tlearn: 0.4708459\ttest: 0.4739423\tbest: 0.4739423 (1000)\ttotal: 2m 46s\tremaining: 5m 10s\n",
      "1100:\tlearn: 0.4697380\ttest: 0.4731047\tbest: 0.4731047 (1100)\ttotal: 3m 3s\tremaining: 4m 54s\n",
      "1200:\tlearn: 0.4687538\ttest: 0.4724308\tbest: 0.4724308 (1200)\ttotal: 3m 20s\tremaining: 4m 38s\n",
      "1300:\tlearn: 0.4678944\ttest: 0.4718676\tbest: 0.4718676 (1300)\ttotal: 3m 38s\tremaining: 4m 22s\n",
      "1400:\tlearn: 0.4671313\ttest: 0.4714425\tbest: 0.4714425 (1400)\ttotal: 3m 54s\tremaining: 4m 5s\n",
      "1500:\tlearn: 0.4664381\ttest: 0.4710650\tbest: 0.4710650 (1500)\ttotal: 4m 13s\tremaining: 3m 50s\n",
      "1600:\tlearn: 0.4657876\ttest: 0.4707609\tbest: 0.4707609 (1600)\ttotal: 4m 31s\tremaining: 3m 34s\n",
      "1700:\tlearn: 0.4652001\ttest: 0.4705055\tbest: 0.4705042 (1697)\ttotal: 4m 48s\tremaining: 3m 17s\n",
      "1800:\tlearn: 0.4646275\ttest: 0.4702587\tbest: 0.4702581 (1799)\ttotal: 5m 7s\tremaining: 3m 2s\n",
      "1900:\tlearn: 0.4641019\ttest: 0.4700727\tbest: 0.4700727 (1900)\ttotal: 5m 26s\tremaining: 2m 45s\n",
      "2000:\tlearn: 0.4635943\ttest: 0.4699223\tbest: 0.4699223 (2000)\ttotal: 5m 46s\tremaining: 2m 29s\n",
      "2100:\tlearn: 0.4631108\ttest: 0.4697884\tbest: 0.4697884 (2100)\ttotal: 6m 5s\tremaining: 2m 12s\n",
      "2200:\tlearn: 0.4626346\ttest: 0.4696292\tbest: 0.4696280 (2199)\ttotal: 6m 21s\tremaining: 1m 55s\n",
      "2300:\tlearn: 0.4621841\ttest: 0.4695334\tbest: 0.4695330 (2299)\ttotal: 6m 38s\tremaining: 1m 37s\n",
      "2400:\tlearn: 0.4617373\ttest: 0.4694637\tbest: 0.4694624 (2396)\ttotal: 6m 54s\tremaining: 1m 20s\n",
      "2500:\tlearn: 0.4613065\ttest: 0.4693903\tbest: 0.4693903 (2500)\ttotal: 7m 10s\tremaining: 1m 2s\n",
      "2600:\tlearn: 0.4608679\ttest: 0.4692982\tbest: 0.4692982 (2600)\ttotal: 7m 25s\tremaining: 45.4s\n",
      "2700:\tlearn: 0.4604520\ttest: 0.4692610\tbest: 0.4692595 (2698)\ttotal: 7m 40s\tremaining: 28.1s\n",
      "2800:\tlearn: 0.4600501\ttest: 0.4692250\tbest: 0.4692250 (2800)\ttotal: 7m 56s\tremaining: 11s\n",
      "2865:\tlearn: 0.4597831\ttest: 0.4691996\tbest: 0.4691996 (2865)\ttotal: 8m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.4691996123\n",
      "bestIteration = 2865\n",
      "\n",
      "auc: 0.855288\n",
      "Fold:1\n",
      "0:\tlearn: 0.6805164\ttest: 0.6805153\tbest: 0.6805153 (0)\ttotal: 213ms\tremaining: 10m 10s\n",
      "100:\tlearn: 0.5173979\ttest: 0.5169653\tbest: 0.5169653 (100)\ttotal: 18.8s\tremaining: 8m 34s\n",
      "200:\tlearn: 0.5053429\ttest: 0.5048380\tbest: 0.5048380 (200)\ttotal: 36.9s\tremaining: 8m 8s\n",
      "300:\tlearn: 0.4977944\ttest: 0.4972211\tbest: 0.4972211 (300)\ttotal: 55.2s\tremaining: 7m 50s\n",
      "400:\tlearn: 0.4884522\ttest: 0.4879065\tbest: 0.4879065 (400)\ttotal: 1m 13s\tremaining: 7m 32s\n",
      "500:\tlearn: 0.4822408\ttest: 0.4817747\tbest: 0.4817747 (500)\ttotal: 1m 31s\tremaining: 7m 13s\n",
      "600:\tlearn: 0.4785030\ttest: 0.4782846\tbest: 0.4782846 (600)\ttotal: 1m 49s\tremaining: 6m 52s\n",
      "700:\tlearn: 0.4759024\ttest: 0.4759544\tbest: 0.4759544 (700)\ttotal: 2m 9s\tremaining: 6m 40s\n",
      "800:\tlearn: 0.4739290\ttest: 0.4742245\tbest: 0.4742245 (800)\ttotal: 2m 30s\tremaining: 6m 27s\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=10,random_state=2020,shuffle=True)\n",
    "cat_test_pred = np.zeros(len(test))\n",
    "cat_train_meta_pred = np.zeros(len(y_train_meta))\n",
    "\n",
    "for fold,(train_idx, val_idx) in enumerate(folds.split(X=X_train,y=y_train)):\n",
    "    print(f'Fold:{fold}')\n",
    "    #create pool objects\n",
    "    train = Pool(data= X_train.iloc[train_idx,:], label=y_train[train_idx])\n",
    "    val = Pool(data= X_train.iloc[val_idx,:], label=y_train[val_idx])\n",
    "    \n",
    "    #create cat model object and train\n",
    "    model_cat = CatBoostClassifier(**cat_params)\n",
    "    model_cat.fit(X=train,\n",
    "        eval_set=val,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=100,\n",
    "    )\n",
    "\n",
    "    #predictions for train_meta and bagging\n",
    "    train_meta_pred_fold = model_cat.predict(X_train_meta)\n",
    "    cat_train_meta_pred += train_meta_pred_fold/folds.n_splits\n",
    "    \n",
    "    #prediction for test and bagging\n",
    "    test_pred_fold = model_cat.predict(X_test)\n",
    "    cat_test_pred += test_pred_fold/folds.n_splits\n",
    "    \n",
    "    #roc_auc_score using training CV for personal satisfaction XD\n",
    "    cv_pred = model_cat.predict_proba(val)[:,-1:]\n",
    "    auc = roc_auc_score(y_train[val_idx],cv_pred)\n",
    "    print(f\"auc: {auc:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTOML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_model = AutoML()\n",
    "automl_model.fit(X_train,y_train,metric='roc_auc', time_budget=5*3600,verbose=2) # ~5 HOURS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_train_meta_pred = automl_model.predict(X_train_meta)\n",
    "automl_test_pred = automl_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auto_ml_model.sav\", 'wb') as file:  \n",
    "    auto_ml = pickle.save(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_test_pred = auto_ml.predict_proba(X_test)[:,-1]\n",
    "automl_cv_pred = auto_ml.predict_proba(X_cv)[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEVEL2 TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X_train_meta = pd.DataFrame(np.column_stack((lgb_train_meta_pred,cat_train_meta_pred ,automl_train_meta_pred)))\n",
    "pred_test = np.column_stack((lgb_test_pred,cat_test_pred,automl_test_pred))\n",
    "print(pred_cv.shape, pred_test.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have npt trained automl yet\n",
    "X_train_meta = np.column_stack((lgb_train_meta_pred,cat_train_meta_pred ))\n",
    "pred_test = np.column_stack((lgb_test_pred,cat_test_pred))\n",
    "print(pred_cv.shape, pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=10,random_state=2021,shuffle=True)\n",
    "final_pred = np.zeros(len(pred_test))\n",
    "\n",
    "for fold,(train_idx, val_idx) in enumerate(folds.split(X_train_meta,y_train_meta)):\n",
    "    print(f'Fold:{fold}')\n",
    "    #create training and CV set for training Lgbm\n",
    "    training = [X_train_meta.iloc[train_idx], label=y_train_meta[train_idx]]\n",
    "    CV = [X_train_meta.iloc[val_idx], label=y_train_meta[val_idx]]\n",
    "    \n",
    "    #create cat model object and train\n",
    "    model_ = LogisticRegression(n_jobs=-1, random_state=42, C=5, max_iter=2000)\n",
    "    model.fit(training[0], training[1])\n",
    "    \n",
    "    #prediction for test and bagging\n",
    "    test_pred_fold = model_cat.predict(pred_test)\n",
    "    final_pred += test_pred_fold/folds.n_splits\n",
    "    \n",
    "    #roc_auc_score using training CV for personal satisfaction XD\n",
    "    cv_pred = model_cat.predict(CV[0])\n",
    "    auc = roc_auc_score(CV[1],cv_pred)\n",
    "    print(f\"auc: {auc:.6f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([pd.DataFrame(ID_test),final_pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred.save_model('Model_{}'.format(time.time()),index=False, header=['id','target'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
